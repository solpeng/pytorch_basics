{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5004356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "====== Epoch:  1 / 10 ======\n",
      "Batch: 10/1000  Loss: 2.302355\n",
      "Batch: 20/1000  Loss: 2.296149\n",
      "Batch: 30/1000  Loss: 2.268746\n",
      "Batch: 40/1000  Loss: 2.145165\n",
      "Batch: 50/1000  Loss: 2.248774\n",
      "Batch: 60/1000  Loss: 2.156821\n",
      "Batch: 70/1000  Loss: 2.118255\n",
      "Batch: 80/1000  Loss: 1.831013\n",
      "Batch: 90/1000  Loss: 2.266618\n",
      "Batch: 100/1000  Loss: 2.069404\n",
      "Batch: 110/1000  Loss: 1.882095\n",
      "Batch: 120/1000  Loss: 1.970169\n",
      "Batch: 130/1000  Loss: 2.107259\n",
      "Batch: 140/1000  Loss: 1.897148\n",
      "Batch: 150/1000  Loss: 2.039683\n",
      "Batch: 160/1000  Loss: 2.051947\n",
      "Batch: 170/1000  Loss: 1.913484\n",
      "Batch: 180/1000  Loss: 1.885376\n",
      "Batch: 190/1000  Loss: 1.611502\n",
      "Batch: 200/1000  Loss: 1.739829\n",
      "Batch: 210/1000  Loss: 2.132336\n",
      "Batch: 220/1000  Loss: 1.872237\n",
      "Batch: 230/1000  Loss: 1.742311\n",
      "Batch: 240/1000  Loss: 1.798359\n",
      "Batch: 250/1000  Loss: 1.818119\n",
      "Batch: 260/1000  Loss: 1.507444\n",
      "Batch: 270/1000  Loss: 1.646263\n",
      "Batch: 280/1000  Loss: 1.538991\n",
      "Batch: 290/1000  Loss: 1.630717\n",
      "Batch: 300/1000  Loss: 1.655787\n",
      "Batch: 310/1000  Loss: 1.386272\n",
      "Batch: 320/1000  Loss: 1.994235\n",
      "Batch: 330/1000  Loss: 1.783011\n",
      "Batch: 340/1000  Loss: 1.815539\n",
      "Batch: 350/1000  Loss: 1.577091\n",
      "Batch: 360/1000  Loss: 1.518605\n",
      "Batch: 370/1000  Loss: 1.491816\n",
      "Batch: 380/1000  Loss: 1.657245\n",
      "Batch: 390/1000  Loss: 1.606475\n",
      "Batch: 400/1000  Loss: 1.508947\n",
      "Batch: 410/1000  Loss: 1.618072\n",
      "Batch: 420/1000  Loss: 1.390535\n",
      "Batch: 430/1000  Loss: 1.432160\n",
      "Batch: 440/1000  Loss: 1.576212\n",
      "Batch: 450/1000  Loss: 1.495756\n",
      "Batch: 460/1000  Loss: 1.343900\n",
      "Batch: 470/1000  Loss: 1.665073\n",
      "Batch: 480/1000  Loss: 1.581392\n",
      "Batch: 490/1000  Loss: 1.502130\n",
      "Batch: 500/1000  Loss: 1.724790\n",
      "Batch: 510/1000  Loss: 1.480509\n",
      "Batch: 520/1000  Loss: 1.369983\n",
      "Batch: 530/1000  Loss: 1.572131\n",
      "Batch: 540/1000  Loss: 1.651792\n",
      "Batch: 550/1000  Loss: 1.292655\n",
      "Batch: 560/1000  Loss: 1.786508\n",
      "Batch: 570/1000  Loss: 1.367831\n",
      "Batch: 580/1000  Loss: 1.417863\n",
      "Batch: 590/1000  Loss: 1.339473\n",
      "Batch: 600/1000  Loss: 1.635116\n",
      "Batch: 610/1000  Loss: 1.763421\n",
      "Batch: 620/1000  Loss: 1.471278\n",
      "Batch: 630/1000  Loss: 1.405257\n",
      "Batch: 640/1000  Loss: 1.222950\n",
      "Batch: 650/1000  Loss: 1.423478\n",
      "Batch: 660/1000  Loss: 1.289959\n",
      "Batch: 670/1000  Loss: 1.199464\n",
      "Batch: 680/1000  Loss: 1.454515\n",
      "Batch: 690/1000  Loss: 1.449319\n",
      "Batch: 700/1000  Loss: 1.079344\n",
      "Batch: 710/1000  Loss: 1.175541\n",
      "Batch: 720/1000  Loss: 1.281907\n",
      "Batch: 730/1000  Loss: 1.144251\n",
      "Batch: 740/1000  Loss: 1.311378\n",
      "Batch: 750/1000  Loss: 1.257907\n",
      "Batch: 760/1000  Loss: 1.472874\n",
      "Batch: 770/1000  Loss: 1.286065\n",
      "Batch: 780/1000  Loss: 1.511215\n",
      "Batch: 790/1000  Loss: 1.233730\n",
      "Batch: 800/1000  Loss: 1.151672\n",
      "Batch: 810/1000  Loss: 1.292737\n",
      "Batch: 820/1000  Loss: 1.134935\n",
      "Batch: 830/1000  Loss: 1.445082\n",
      "Batch: 840/1000  Loss: 1.280036\n",
      "Batch: 850/1000  Loss: 1.178686\n",
      "Batch: 860/1000  Loss: 1.313703\n",
      "Batch: 870/1000  Loss: 1.108791\n",
      "Batch: 880/1000  Loss: 1.622750\n",
      "Batch: 890/1000  Loss: 1.246181\n",
      "Batch: 900/1000  Loss: 1.401046\n",
      "Batch: 910/1000  Loss: 1.178961\n",
      "Batch: 920/1000  Loss: 1.166216\n",
      "Batch: 930/1000  Loss: 1.276927\n",
      "Batch: 940/1000  Loss: 1.252449\n",
      "Batch: 950/1000  Loss: 1.051573\n",
      "Batch: 960/1000  Loss: 1.106917\n",
      "Batch: 970/1000  Loss: 1.032303\n",
      "Batch: 980/1000  Loss: 1.266583\n",
      "Batch: 990/1000  Loss: 1.210664\n",
      "Batch: 1000/1000  Loss: 1.252496\n",
      "Accuracy: 58.67%\n",
      "elapsed time: 266.9497630596161\n",
      "====== Epoch:  2 / 10 ======\n",
      "Batch: 10/1000  Loss: 1.071447\n",
      "Batch: 20/1000  Loss: 1.124524\n",
      "Batch: 30/1000  Loss: 1.324905\n",
      "Batch: 40/1000  Loss: 1.113292\n",
      "Batch: 50/1000  Loss: 1.305607\n",
      "Batch: 60/1000  Loss: 1.203286\n",
      "Batch: 70/1000  Loss: 1.242531\n",
      "Batch: 80/1000  Loss: 1.191258\n",
      "Batch: 90/1000  Loss: 1.200501\n",
      "Batch: 100/1000  Loss: 1.176719\n",
      "Batch: 110/1000  Loss: 1.035598\n",
      "Batch: 120/1000  Loss: 1.227288\n",
      "Batch: 130/1000  Loss: 0.856606\n",
      "Batch: 140/1000  Loss: 0.887086\n",
      "Batch: 150/1000  Loss: 0.943923\n",
      "Batch: 160/1000  Loss: 1.054330\n",
      "Batch: 170/1000  Loss: 1.133663\n",
      "Batch: 180/1000  Loss: 1.350947\n",
      "Batch: 190/1000  Loss: 1.255944\n",
      "Batch: 200/1000  Loss: 0.931707\n",
      "Batch: 210/1000  Loss: 0.655803\n",
      "Batch: 220/1000  Loss: 1.142774\n",
      "Batch: 230/1000  Loss: 0.906178\n",
      "Batch: 240/1000  Loss: 1.103578\n",
      "Batch: 250/1000  Loss: 1.175508\n",
      "Batch: 260/1000  Loss: 0.960545\n",
      "Batch: 270/1000  Loss: 1.050683\n",
      "Batch: 280/1000  Loss: 0.703352\n",
      "Batch: 290/1000  Loss: 0.870680\n",
      "Batch: 300/1000  Loss: 0.926451\n",
      "Batch: 310/1000  Loss: 1.295175\n",
      "Batch: 320/1000  Loss: 0.911851\n",
      "Batch: 330/1000  Loss: 1.116358\n",
      "Batch: 340/1000  Loss: 0.946968\n",
      "Batch: 350/1000  Loss: 0.721384\n",
      "Batch: 360/1000  Loss: 0.955346\n",
      "Batch: 370/1000  Loss: 1.020710\n",
      "Batch: 380/1000  Loss: 0.811885\n",
      "Batch: 390/1000  Loss: 1.036670\n",
      "Batch: 400/1000  Loss: 0.858697\n",
      "Batch: 410/1000  Loss: 0.841869\n",
      "Batch: 420/1000  Loss: 0.809361\n",
      "Batch: 430/1000  Loss: 1.050480\n",
      "Batch: 440/1000  Loss: 1.071917\n",
      "Batch: 450/1000  Loss: 1.133217\n",
      "Batch: 460/1000  Loss: 0.767546\n",
      "Batch: 470/1000  Loss: 1.137722\n",
      "Batch: 480/1000  Loss: 0.995724\n",
      "Batch: 490/1000  Loss: 1.043219\n",
      "Batch: 500/1000  Loss: 0.783664\n",
      "Batch: 510/1000  Loss: 1.099559\n",
      "Batch: 520/1000  Loss: 1.120084\n",
      "Batch: 530/1000  Loss: 1.102028\n",
      "Batch: 540/1000  Loss: 0.579894\n",
      "Batch: 550/1000  Loss: 0.886479\n",
      "Batch: 560/1000  Loss: 0.893049\n",
      "Batch: 570/1000  Loss: 0.842321\n",
      "Batch: 580/1000  Loss: 1.108941\n",
      "Batch: 590/1000  Loss: 0.928283\n",
      "Batch: 600/1000  Loss: 0.975728\n",
      "Batch: 610/1000  Loss: 0.974566\n",
      "Batch: 620/1000  Loss: 1.049706\n",
      "Batch: 630/1000  Loss: 0.860937\n",
      "Batch: 640/1000  Loss: 1.017356\n",
      "Batch: 650/1000  Loss: 0.850894\n",
      "Batch: 660/1000  Loss: 0.878673\n",
      "Batch: 670/1000  Loss: 0.724708\n",
      "Batch: 680/1000  Loss: 0.703391\n",
      "Batch: 690/1000  Loss: 1.056818\n",
      "Batch: 700/1000  Loss: 0.815532\n",
      "Batch: 710/1000  Loss: 0.824674\n",
      "Batch: 720/1000  Loss: 1.008523\n",
      "Batch: 730/1000  Loss: 0.664666\n",
      "Batch: 740/1000  Loss: 0.996465\n",
      "Batch: 750/1000  Loss: 0.899933\n",
      "Batch: 760/1000  Loss: 0.666365\n",
      "Batch: 770/1000  Loss: 1.185134\n",
      "Batch: 780/1000  Loss: 0.883613\n",
      "Batch: 790/1000  Loss: 0.884828\n",
      "Batch: 800/1000  Loss: 0.759844\n",
      "Batch: 810/1000  Loss: 0.626867\n",
      "Batch: 820/1000  Loss: 0.640960\n",
      "Batch: 830/1000  Loss: 0.524970\n",
      "Batch: 840/1000  Loss: 0.540391\n",
      "Batch: 850/1000  Loss: 0.872145\n",
      "Batch: 860/1000  Loss: 0.890466\n",
      "Batch: 870/1000  Loss: 0.906403\n",
      "Batch: 880/1000  Loss: 0.791846\n",
      "Batch: 890/1000  Loss: 0.955309\n",
      "Batch: 900/1000  Loss: 0.752297\n",
      "Batch: 910/1000  Loss: 0.695016\n",
      "Batch: 920/1000  Loss: 0.925032\n",
      "Batch: 930/1000  Loss: 0.908016\n",
      "Batch: 940/1000  Loss: 1.233711\n",
      "Batch: 950/1000  Loss: 0.753209\n",
      "Batch: 960/1000  Loss: 0.732052\n",
      "Batch: 970/1000  Loss: 0.965440\n",
      "Batch: 980/1000  Loss: 0.887089\n",
      "Batch: 990/1000  Loss: 1.092743\n",
      "Batch: 1000/1000  Loss: 0.680839\n",
      "Accuracy: 72.98%\n",
      "elapsed time: 266.71066546440125\n",
      "====== Epoch:  3 / 10 ======\n",
      "Batch: 10/1000  Loss: 0.855807\n",
      "Batch: 20/1000  Loss: 0.703446\n",
      "Batch: 30/1000  Loss: 0.953816\n",
      "Batch: 40/1000  Loss: 0.466489\n",
      "Batch: 50/1000  Loss: 0.896003\n",
      "Batch: 60/1000  Loss: 0.562619\n",
      "Batch: 70/1000  Loss: 0.508998\n",
      "Batch: 80/1000  Loss: 0.431933\n",
      "Batch: 90/1000  Loss: 0.749244\n",
      "Batch: 100/1000  Loss: 0.523339\n",
      "Batch: 110/1000  Loss: 0.494884\n",
      "Batch: 120/1000  Loss: 0.817209\n",
      "Batch: 130/1000  Loss: 0.609089\n",
      "Batch: 140/1000  Loss: 0.575174\n",
      "Batch: 150/1000  Loss: 0.707034\n",
      "Batch: 160/1000  Loss: 0.653746\n",
      "Batch: 170/1000  Loss: 0.673025\n",
      "Batch: 180/1000  Loss: 0.883559\n",
      "Batch: 190/1000  Loss: 0.576539\n",
      "Batch: 200/1000  Loss: 0.600748\n",
      "Batch: 210/1000  Loss: 0.609370\n",
      "Batch: 220/1000  Loss: 0.772871\n",
      "Batch: 230/1000  Loss: 0.820801\n",
      "Batch: 240/1000  Loss: 0.622337\n",
      "Batch: 250/1000  Loss: 0.609215\n",
      "Batch: 260/1000  Loss: 0.659611\n",
      "Batch: 270/1000  Loss: 0.447394\n",
      "Batch: 280/1000  Loss: 0.646238\n",
      "Batch: 290/1000  Loss: 0.643738\n",
      "Batch: 300/1000  Loss: 0.654035\n",
      "Batch: 310/1000  Loss: 0.615320\n",
      "Batch: 320/1000  Loss: 0.568051\n",
      "Batch: 330/1000  Loss: 0.726525\n",
      "Batch: 340/1000  Loss: 0.531962\n",
      "Batch: 350/1000  Loss: 0.570422\n",
      "Batch: 360/1000  Loss: 0.428741\n",
      "Batch: 370/1000  Loss: 0.396208\n",
      "Batch: 380/1000  Loss: 0.479223\n",
      "Batch: 390/1000  Loss: 0.566128\n",
      "Batch: 400/1000  Loss: 0.688022\n",
      "Batch: 410/1000  Loss: 0.454780\n",
      "Batch: 420/1000  Loss: 0.597950\n",
      "Batch: 430/1000  Loss: 0.644667\n",
      "Batch: 440/1000  Loss: 0.507785\n",
      "Batch: 450/1000  Loss: 0.535442\n",
      "Batch: 460/1000  Loss: 0.237020\n",
      "Batch: 470/1000  Loss: 0.674367\n",
      "Batch: 480/1000  Loss: 0.586564\n",
      "Batch: 490/1000  Loss: 0.491350\n",
      "Batch: 500/1000  Loss: 0.452946\n",
      "Batch: 510/1000  Loss: 0.801996\n",
      "Batch: 520/1000  Loss: 0.685647\n",
      "Batch: 530/1000  Loss: 0.734123\n",
      "Batch: 540/1000  Loss: 0.658077\n",
      "Batch: 550/1000  Loss: 0.863953\n",
      "Batch: 560/1000  Loss: 0.680440\n",
      "Batch: 570/1000  Loss: 0.577931\n",
      "Batch: 580/1000  Loss: 0.720691\n",
      "Batch: 590/1000  Loss: 0.934101\n",
      "Batch: 600/1000  Loss: 0.804511\n",
      "Batch: 610/1000  Loss: 0.579674\n",
      "Batch: 620/1000  Loss: 0.670717\n",
      "Batch: 630/1000  Loss: 0.681002\n",
      "Batch: 640/1000  Loss: 0.565125\n",
      "Batch: 650/1000  Loss: 0.701017\n",
      "Batch: 660/1000  Loss: 0.702304\n",
      "Batch: 670/1000  Loss: 0.521781\n",
      "Batch: 680/1000  Loss: 0.670792\n",
      "Batch: 690/1000  Loss: 0.641557\n",
      "Batch: 700/1000  Loss: 0.720202\n",
      "Batch: 710/1000  Loss: 0.650358\n",
      "Batch: 720/1000  Loss: 0.465837\n",
      "Batch: 730/1000  Loss: 0.773337\n",
      "Batch: 740/1000  Loss: 0.580906\n",
      "Batch: 750/1000  Loss: 0.303834\n",
      "Batch: 760/1000  Loss: 0.491973\n",
      "Batch: 770/1000  Loss: 0.730929\n",
      "Batch: 780/1000  Loss: 0.926086\n",
      "Batch: 790/1000  Loss: 0.306205\n",
      "Batch: 800/1000  Loss: 0.518312\n",
      "Batch: 810/1000  Loss: 0.543181\n",
      "Batch: 820/1000  Loss: 0.574524\n",
      "Batch: 830/1000  Loss: 0.616432\n",
      "Batch: 840/1000  Loss: 0.537936\n",
      "Batch: 850/1000  Loss: 0.290084\n",
      "Batch: 860/1000  Loss: 0.607976\n",
      "Batch: 870/1000  Loss: 0.658089\n",
      "Batch: 880/1000  Loss: 0.520220\n",
      "Batch: 890/1000  Loss: 0.488203\n",
      "Batch: 900/1000  Loss: 0.488224\n",
      "Batch: 910/1000  Loss: 0.622767\n",
      "Batch: 920/1000  Loss: 0.753285\n",
      "Batch: 930/1000  Loss: 0.473233\n",
      "Batch: 940/1000  Loss: 0.565735\n",
      "Batch: 950/1000  Loss: 0.469116\n",
      "Batch: 960/1000  Loss: 0.551174\n",
      "Batch: 970/1000  Loss: 0.503739\n",
      "Batch: 980/1000  Loss: 0.470377\n",
      "Batch: 990/1000  Loss: 0.348123\n",
      "Batch: 1000/1000  Loss: 0.486686\n",
      "Accuracy: 75.93%\n",
      "elapsed time: 266.76571130752563\n",
      "====== Epoch:  4 / 10 ======\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    159\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    160\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 161\u001b[0m l_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    164\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)        \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGGNet, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512*7*7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        z = self.classifier(x)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = T.Compose([T.ToTensor(),\n",
    "                       T.Resize((224,224))])\n",
    "\n",
    "train_set = CIFAR10(root='CIFAR10_data/',\n",
    "                    train=True,\n",
    "                    transform=transform,\n",
    "                    download=True)\n",
    "test_set = CIFAR10(root='CIFAR10_data/',\n",
    "                   train=False,\n",
    "                   transform=transform,\n",
    "                   download=True)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=12)\n",
    "test_loader = DataLoader(dataset=test_set,\n",
    "                          batch_size=10,\n",
    "                          shuffle=False,\n",
    "                          num_workers=0)\n",
    "\n",
    "\n",
    "net = VGGNet(num_classes=10).to(DEVICE)\n",
    "\n",
    "cel = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "EPOCHS = 10\n",
    "loss_lst = []\n",
    "acc_lst = []\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    batch_time = time.time()\n",
    "    print(f'====== Epoch: {epoch+1:2d} / {EPOCHS} ======')\n",
    "    net.train()\n",
    "    l_sum = 0\n",
    "    for batch_idx, (x,y) in enumerate(train_loader):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        z = net(x)\n",
    "        loss = cel(z, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        l_sum += loss.item()\n",
    "        if (batch_idx+1) % 10 == 0:\n",
    "            print(f'Batch: {batch_idx+1:2d}/{len(train_loader)} ',\n",
    "                  f'Loss: {loss.item():0.6f}')       \n",
    "\n",
    "    loss_lst.append(l_sum/len(train_loader))\n",
    "\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x,y) in enumerate(test_loader):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            z = net(x)\n",
    "            yhat = torch.argmax(z, dim=1)\n",
    "            correct += torch.sum(y==yhat)\n",
    "    \n",
    "    accuracy = correct / len(test_set)\n",
    "    acc_lst.append(accuracy)\n",
    "    print(f'Accuracy: {accuracy.item()*100:0.2f}%')\n",
    "    print(\"elapsed time:\", time.time() - batch_time)\n",
    "        \n",
    "print(\"elapsed time:\", time.time() - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hansol",
   "language": "python",
   "name": "hansol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
