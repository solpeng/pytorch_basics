{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "====== Epoch:  1 / 10 ======\n",
      "Batch: 10/1000  Loss: 1.936302\n",
      "Batch: 20/1000  Loss: 1.582902\n",
      "Batch: 30/1000  Loss: 1.302579\n",
      "Batch: 40/1000  Loss: 0.981182\n",
      "Batch: 50/1000  Loss: 0.957966\n",
      "Batch: 60/1000  Loss: 1.163240\n",
      "Batch: 70/1000  Loss: 1.009224\n",
      "Batch: 80/1000  Loss: 0.584713\n",
      "Batch: 90/1000  Loss: 0.889232\n",
      "Batch: 100/1000  Loss: 0.713211\n",
      "Batch: 110/1000  Loss: 0.527913\n",
      "Batch: 120/1000  Loss: 0.863501\n",
      "Batch: 130/1000  Loss: 0.604608\n",
      "Batch: 140/1000  Loss: 0.475462\n",
      "Batch: 150/1000  Loss: 0.759447\n",
      "Batch: 160/1000  Loss: 0.461535\n",
      "Batch: 170/1000  Loss: 0.424075\n",
      "Batch: 180/1000  Loss: 0.728401\n",
      "Batch: 190/1000  Loss: 0.519665\n",
      "Batch: 200/1000  Loss: 0.666761\n",
      "Batch: 210/1000  Loss: 0.854255\n",
      "Batch: 220/1000  Loss: 0.270021\n",
      "Batch: 230/1000  Loss: 0.463320\n",
      "Batch: 240/1000  Loss: 0.464961\n",
      "Batch: 250/1000  Loss: 0.337852\n",
      "Batch: 260/1000  Loss: 0.565331\n",
      "Batch: 270/1000  Loss: 0.400102\n",
      "Batch: 280/1000  Loss: 0.424279\n",
      "Batch: 290/1000  Loss: 0.680034\n",
      "Batch: 300/1000  Loss: 0.742945\n",
      "Batch: 310/1000  Loss: 0.256103\n",
      "Batch: 320/1000  Loss: 0.379571\n",
      "Batch: 330/1000  Loss: 0.287373\n",
      "Batch: 340/1000  Loss: 0.382319\n",
      "Batch: 350/1000  Loss: 0.309198\n",
      "Batch: 360/1000  Loss: 0.639272\n",
      "Batch: 370/1000  Loss: 0.439300\n",
      "Batch: 380/1000  Loss: 0.614334\n",
      "Batch: 390/1000  Loss: 0.488607\n",
      "Batch: 400/1000  Loss: 0.561468\n",
      "Batch: 410/1000  Loss: 0.521444\n",
      "Batch: 420/1000  Loss: 0.163234\n",
      "Batch: 430/1000  Loss: 0.356129\n",
      "Batch: 440/1000  Loss: 0.327034\n",
      "Batch: 450/1000  Loss: 0.470243\n",
      "Batch: 460/1000  Loss: 0.453048\n",
      "Batch: 470/1000  Loss: 0.636346\n",
      "Batch: 480/1000  Loss: 0.295855\n",
      "Batch: 490/1000  Loss: 0.379050\n",
      "Batch: 500/1000  Loss: 0.309057\n",
      "Batch: 510/1000  Loss: 0.272899\n",
      "Batch: 520/1000  Loss: 0.277909\n",
      "Batch: 530/1000  Loss: 0.581254\n",
      "Batch: 540/1000  Loss: 0.275065\n",
      "Batch: 550/1000  Loss: 0.207169\n",
      "Batch: 560/1000  Loss: 0.428219\n",
      "Batch: 570/1000  Loss: 0.355032\n",
      "Batch: 580/1000  Loss: 0.492149\n",
      "Batch: 590/1000  Loss: 0.320855\n",
      "Batch: 600/1000  Loss: 0.138115\n",
      "Batch: 610/1000  Loss: 0.144289\n",
      "Batch: 620/1000  Loss: 0.464747\n",
      "Batch: 630/1000  Loss: 0.571752\n",
      "Batch: 640/1000  Loss: 0.387360\n",
      "Batch: 650/1000  Loss: 0.349621\n",
      "Batch: 660/1000  Loss: 0.286542\n",
      "Batch: 670/1000  Loss: 0.293329\n",
      "Batch: 680/1000  Loss: 0.270930\n",
      "Batch: 690/1000  Loss: 0.579171\n",
      "Batch: 700/1000  Loss: 0.373145\n",
      "Batch: 710/1000  Loss: 0.300786\n",
      "Batch: 720/1000  Loss: 0.570756\n",
      "Batch: 730/1000  Loss: 0.312857\n",
      "Batch: 740/1000  Loss: 0.299015\n",
      "Batch: 750/1000  Loss: 0.325302\n",
      "Batch: 760/1000  Loss: 0.277685\n",
      "Batch: 770/1000  Loss: 0.549658\n",
      "Batch: 780/1000  Loss: 0.322147\n",
      "Batch: 790/1000  Loss: 0.277268\n",
      "Batch: 800/1000  Loss: 0.228109\n",
      "Batch: 810/1000  Loss: 0.218961\n",
      "Batch: 820/1000  Loss: 0.493319\n",
      "Batch: 830/1000  Loss: 0.246573\n",
      "Batch: 840/1000  Loss: 0.396386\n",
      "Batch: 850/1000  Loss: 0.391911\n",
      "Batch: 860/1000  Loss: 0.152878\n",
      "Batch: 870/1000  Loss: 0.256915\n",
      "Batch: 880/1000  Loss: 0.239847\n",
      "Batch: 890/1000  Loss: 0.536693\n",
      "Batch: 900/1000  Loss: 0.516974\n",
      "Batch: 910/1000  Loss: 0.295944\n",
      "Batch: 920/1000  Loss: 0.358752\n",
      "Batch: 930/1000  Loss: 0.213934\n",
      "Batch: 940/1000  Loss: 0.388845\n",
      "Batch: 950/1000  Loss: 0.431921\n",
      "Batch: 960/1000  Loss: 0.405765\n",
      "Batch: 970/1000  Loss: 0.277675\n",
      "Batch: 980/1000  Loss: 0.361831\n",
      "Batch: 990/1000  Loss: 0.489147\n",
      "Batch: 1000/1000  Loss: 0.341001\n",
      "Accuracy: 89.06%\n",
      "elapsed time: 267.348370552063\n",
      "====== Epoch:  2 / 10 ======\n",
      "Batch: 10/1000  Loss: 0.327437\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 58\u001b[0m l_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)        \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = T.Compose([T.ToTensor(),\n",
    "                       T.Resize((224,224))])\n",
    "\n",
    "train_set = CIFAR10(root='CIFAR10_data/',\n",
    "                    train=True,\n",
    "                    transform=transform,\n",
    "                    download=True)\n",
    "test_set = CIFAR10(root='CIFAR10_data/',\n",
    "                   train=False,\n",
    "                   transform=transform,\n",
    "                   download=True)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "test_loader = DataLoader(dataset=test_set,\n",
    "                          batch_size=10,\n",
    "                          shuffle=False,\n",
    "                          num_workers=0)\n",
    "\n",
    "net = vgg19(pretrained=True)\n",
    "net.classifier[6] = torch.nn.Linear(4096, 10)\n",
    "net.to(DEVICE)\n",
    "\n",
    "cel = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "EPOCHS = 10\n",
    "loss_lst = []\n",
    "acc_lst = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    batch_time = time.time()\n",
    "    print(f'====== Epoch: {epoch+1:2d} / {EPOCHS} ======')\n",
    "    net.train()\n",
    "    l_sum = 0\n",
    "    for batch_idx, (x,y) in enumerate(train_loader):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        z = net(x)\n",
    "        loss = cel(z, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        l_sum += loss.item()\n",
    "        if (batch_idx+1) % 10 == 0:\n",
    "            print(f'Batch: {batch_idx+1:2d}/{len(train_loader)} ',\n",
    "                f'Loss: {loss.item():0.6f}')        \n",
    "            \n",
    "    loss_lst.append(l_sum/len(train_loader))\n",
    "\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x,y) in enumerate(test_loader):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            z = net(x)\n",
    "            yhat = torch.argmax(z, dim=1)\n",
    "            correct += torch.sum(y==yhat)\n",
    "        \n",
    "    accuracy = correct / len(test_set)\n",
    "    acc_lst.append(accuracy)\n",
    "    print(f'Accuracy: {accuracy.item()*100:0.2f}%')\n",
    "    print(\"elapsed time:\", time.time() - batch_time)\n",
    "        \n",
    "print(\"elapsed time:\", time.time() - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hansol",
   "language": "python",
   "name": "hansol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
