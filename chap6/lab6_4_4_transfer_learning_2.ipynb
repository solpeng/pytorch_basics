{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "====== Epoch:  1 / 10 ======\n",
      "Batch: 10/1000  Loss: 2.023846\n",
      "Batch: 20/1000  Loss: 1.672120\n",
      "Batch: 30/1000  Loss: 1.108423\n",
      "Batch: 40/1000  Loss: 1.157998\n",
      "Batch: 50/1000  Loss: 1.268990\n",
      "Batch: 60/1000  Loss: 0.961299\n",
      "Batch: 70/1000  Loss: 1.373873\n",
      "Batch: 80/1000  Loss: 1.017216\n",
      "Batch: 90/1000  Loss: 0.836885\n",
      "Batch: 100/1000  Loss: 1.096287\n",
      "Batch: 110/1000  Loss: 0.985194\n",
      "Batch: 120/1000  Loss: 0.917531\n",
      "Batch: 130/1000  Loss: 0.841208\n",
      "Batch: 140/1000  Loss: 0.836402\n",
      "Batch: 150/1000  Loss: 0.710030\n",
      "Batch: 160/1000  Loss: 1.113310\n",
      "Batch: 170/1000  Loss: 1.000322\n",
      "Batch: 180/1000  Loss: 0.960149\n",
      "Batch: 190/1000  Loss: 1.065930\n",
      "Batch: 200/1000  Loss: 0.722316\n",
      "Batch: 210/1000  Loss: 0.698926\n",
      "Batch: 220/1000  Loss: 0.570545\n",
      "Batch: 230/1000  Loss: 0.742801\n",
      "Batch: 240/1000  Loss: 0.640558\n",
      "Batch: 250/1000  Loss: 0.607814\n",
      "Batch: 260/1000  Loss: 0.901059\n",
      "Batch: 270/1000  Loss: 0.936600\n",
      "Batch: 280/1000  Loss: 0.837588\n",
      "Batch: 290/1000  Loss: 0.771966\n",
      "Batch: 300/1000  Loss: 0.727296\n",
      "Batch: 310/1000  Loss: 0.880544\n",
      "Batch: 320/1000  Loss: 0.826744\n",
      "Batch: 330/1000  Loss: 0.652450\n",
      "Batch: 340/1000  Loss: 0.939095\n",
      "Batch: 350/1000  Loss: 0.893998\n",
      "Batch: 360/1000  Loss: 0.951471\n",
      "Batch: 370/1000  Loss: 0.632551\n",
      "Batch: 380/1000  Loss: 0.931292\n",
      "Batch: 390/1000  Loss: 0.691526\n",
      "Batch: 400/1000  Loss: 0.650705\n",
      "Batch: 410/1000  Loss: 0.630075\n",
      "Batch: 420/1000  Loss: 0.565290\n",
      "Batch: 430/1000  Loss: 0.620237\n",
      "Batch: 440/1000  Loss: 0.748935\n",
      "Batch: 450/1000  Loss: 0.979149\n",
      "Batch: 460/1000  Loss: 0.627626\n",
      "Batch: 470/1000  Loss: 0.584260\n",
      "Batch: 480/1000  Loss: 0.578410\n",
      "Batch: 490/1000  Loss: 0.698642\n",
      "Batch: 500/1000  Loss: 0.694279\n",
      "Batch: 510/1000  Loss: 0.709820\n",
      "Batch: 520/1000  Loss: 0.709438\n",
      "Batch: 530/1000  Loss: 0.765302\n",
      "Batch: 540/1000  Loss: 0.667651\n",
      "Batch: 550/1000  Loss: 0.549175\n",
      "Batch: 560/1000  Loss: 0.659264\n",
      "Batch: 570/1000  Loss: 0.797958\n",
      "Batch: 580/1000  Loss: 0.813384\n",
      "Batch: 590/1000  Loss: 1.013871\n",
      "Batch: 600/1000  Loss: 0.834177\n",
      "Batch: 610/1000  Loss: 0.918391\n",
      "Batch: 620/1000  Loss: 0.658523\n",
      "Batch: 630/1000  Loss: 0.778085\n",
      "Batch: 640/1000  Loss: 0.868123\n",
      "Batch: 650/1000  Loss: 0.543949\n",
      "Batch: 660/1000  Loss: 0.722476\n",
      "Batch: 670/1000  Loss: 0.526591\n",
      "Batch: 680/1000  Loss: 0.581508\n",
      "Batch: 690/1000  Loss: 0.696289\n",
      "Batch: 700/1000  Loss: 0.447249\n",
      "Batch: 710/1000  Loss: 0.682870\n",
      "Batch: 720/1000  Loss: 0.578221\n",
      "Batch: 730/1000  Loss: 0.590328\n",
      "Batch: 740/1000  Loss: 0.944195\n",
      "Batch: 750/1000  Loss: 0.531710\n",
      "Batch: 760/1000  Loss: 0.597500\n",
      "Batch: 770/1000  Loss: 0.663766\n",
      "Batch: 780/1000  Loss: 0.786677\n",
      "Batch: 790/1000  Loss: 0.689809\n",
      "Batch: 800/1000  Loss: 0.774015\n",
      "Batch: 810/1000  Loss: 0.557943\n",
      "Batch: 820/1000  Loss: 1.015880\n",
      "Batch: 830/1000  Loss: 0.629702\n",
      "Batch: 840/1000  Loss: 0.563746\n",
      "Batch: 850/1000  Loss: 0.861782\n",
      "Batch: 860/1000  Loss: 0.591144\n",
      "Batch: 870/1000  Loss: 0.890218\n",
      "Batch: 880/1000  Loss: 0.695949\n",
      "Batch: 890/1000  Loss: 0.562619\n",
      "Batch: 900/1000  Loss: 0.528585\n",
      "Batch: 910/1000  Loss: 0.400285\n",
      "Batch: 920/1000  Loss: 0.459284\n",
      "Batch: 930/1000  Loss: 0.870016\n",
      "Batch: 940/1000  Loss: 0.636086\n",
      "Batch: 950/1000  Loss: 0.650754\n",
      "Batch: 960/1000  Loss: 0.697621\n",
      "Batch: 970/1000  Loss: 0.866986\n",
      "Batch: 980/1000  Loss: 0.542794\n",
      "Batch: 990/1000  Loss: 0.684262\n",
      "Batch: 1000/1000  Loss: 0.528276\n",
      "Accuracy: 80.09%\n",
      "elapsed time: 117.3728096485138\n",
      "====== Epoch:  2 / 10 ======\n",
      "Batch: 10/1000  Loss: 0.577255\n",
      "Batch: 20/1000  Loss: 0.286391\n",
      "Batch: 30/1000  Loss: 0.793223\n",
      "Batch: 40/1000  Loss: 0.396724\n",
      "Batch: 50/1000  Loss: 0.552023\n",
      "Batch: 60/1000  Loss: 0.326102\n",
      "Batch: 70/1000  Loss: 0.716464\n",
      "Batch: 80/1000  Loss: 0.520057\n",
      "Batch: 90/1000  Loss: 0.405847\n",
      "Batch: 100/1000  Loss: 0.369937\n",
      "Batch: 110/1000  Loss: 0.308010\n",
      "Batch: 120/1000  Loss: 0.530950\n",
      "Batch: 130/1000  Loss: 0.727039\n",
      "Batch: 140/1000  Loss: 0.459775\n",
      "Batch: 150/1000  Loss: 0.620646\n",
      "Batch: 160/1000  Loss: 0.407529\n",
      "Batch: 170/1000  Loss: 0.694941\n",
      "Batch: 180/1000  Loss: 0.396754\n",
      "Batch: 190/1000  Loss: 0.558673\n",
      "Batch: 200/1000  Loss: 0.341258\n",
      "Batch: 210/1000  Loss: 0.356248\n",
      "Batch: 220/1000  Loss: 0.432987\n",
      "Batch: 230/1000  Loss: 0.548786\n",
      "Batch: 240/1000  Loss: 0.521528\n",
      "Batch: 250/1000  Loss: 0.575235\n",
      "Batch: 260/1000  Loss: 0.395896\n",
      "Batch: 270/1000  Loss: 0.332109\n",
      "Batch: 280/1000  Loss: 0.433914\n",
      "Batch: 290/1000  Loss: 0.544481\n",
      "Batch: 300/1000  Loss: 0.583370\n",
      "Batch: 310/1000  Loss: 0.528710\n",
      "Batch: 320/1000  Loss: 0.382646\n",
      "Batch: 330/1000  Loss: 0.425192\n",
      "Batch: 340/1000  Loss: 0.476973\n",
      "Batch: 350/1000  Loss: 0.300784\n",
      "Batch: 360/1000  Loss: 0.488044\n",
      "Batch: 370/1000  Loss: 0.548298\n",
      "Batch: 380/1000  Loss: 0.618258\n",
      "Batch: 390/1000  Loss: 0.298995\n",
      "Batch: 400/1000  Loss: 0.434497\n",
      "Batch: 410/1000  Loss: 0.499004\n",
      "Batch: 420/1000  Loss: 0.545511\n",
      "Batch: 430/1000  Loss: 0.396779\n",
      "Batch: 440/1000  Loss: 0.445935\n",
      "Batch: 450/1000  Loss: 0.533817\n",
      "Batch: 460/1000  Loss: 0.297095\n",
      "Batch: 470/1000  Loss: 0.263852\n",
      "Batch: 480/1000  Loss: 0.420363\n",
      "Batch: 490/1000  Loss: 0.443244\n",
      "Batch: 500/1000  Loss: 0.249407\n",
      "Batch: 510/1000  Loss: 0.368676\n",
      "Batch: 520/1000  Loss: 0.520484\n",
      "Batch: 530/1000  Loss: 0.513494\n",
      "Batch: 540/1000  Loss: 0.515919\n",
      "Batch: 550/1000  Loss: 0.495570\n",
      "Batch: 560/1000  Loss: 0.395027\n",
      "Batch: 570/1000  Loss: 0.603625\n",
      "Batch: 580/1000  Loss: 0.417674\n",
      "Batch: 590/1000  Loss: 0.532225\n",
      "Batch: 600/1000  Loss: 0.406925\n",
      "Batch: 610/1000  Loss: 0.434586\n",
      "Batch: 620/1000  Loss: 0.423841\n",
      "Batch: 630/1000  Loss: 0.325227\n",
      "Batch: 640/1000  Loss: 0.428198\n",
      "Batch: 650/1000  Loss: 0.509597\n",
      "Batch: 660/1000  Loss: 0.486429\n",
      "Batch: 670/1000  Loss: 0.368971\n",
      "Batch: 680/1000  Loss: 0.512104\n",
      "Batch: 690/1000  Loss: 0.238717\n",
      "Batch: 700/1000  Loss: 0.700859\n",
      "Batch: 710/1000  Loss: 0.579465\n",
      "Batch: 720/1000  Loss: 0.526846\n",
      "Batch: 730/1000  Loss: 0.440659\n",
      "Batch: 740/1000  Loss: 0.255311\n",
      "Batch: 750/1000  Loss: 0.546223\n",
      "Batch: 760/1000  Loss: 0.298560\n",
      "Batch: 770/1000  Loss: 0.391128\n",
      "Batch: 780/1000  Loss: 0.582991\n",
      "Batch: 790/1000  Loss: 0.564980\n",
      "Batch: 800/1000  Loss: 0.394256\n",
      "Batch: 810/1000  Loss: 0.474961\n",
      "Batch: 820/1000  Loss: 0.570017\n",
      "Batch: 830/1000  Loss: 0.548716\n",
      "Batch: 840/1000  Loss: 0.468523\n",
      "Batch: 850/1000  Loss: 0.553220\n",
      "Batch: 860/1000  Loss: 0.340223\n",
      "Batch: 870/1000  Loss: 0.447002\n",
      "Batch: 880/1000  Loss: 0.539919\n",
      "Batch: 890/1000  Loss: 0.436354\n",
      "Batch: 900/1000  Loss: 0.475039\n",
      "Batch: 910/1000  Loss: 0.438474\n",
      "Batch: 920/1000  Loss: 0.366889\n",
      "Batch: 930/1000  Loss: 0.438295\n",
      "Batch: 940/1000  Loss: 0.372832\n",
      "Batch: 950/1000  Loss: 0.370720\n",
      "Batch: 960/1000  Loss: 0.460251\n",
      "Batch: 970/1000  Loss: 0.449117\n",
      "Batch: 980/1000  Loss: 0.437660\n",
      "Batch: 990/1000  Loss: 0.536425\n",
      "Batch: 1000/1000  Loss: 0.599741\n",
      "Accuracy: 79.68%\n",
      "elapsed time: 117.31472420692444\n",
      "====== Epoch:  3 / 10 ======\n",
      "Batch: 10/1000  Loss: 0.380370\n",
      "Batch: 20/1000  Loss: 0.215326\n",
      "Batch: 30/1000  Loss: 0.250427\n",
      "Batch: 40/1000  Loss: 0.217098\n",
      "Batch: 50/1000  Loss: 0.251090\n",
      "Batch: 60/1000  Loss: 0.248723\n",
      "Batch: 70/1000  Loss: 0.308669\n",
      "Batch: 80/1000  Loss: 0.183253\n",
      "Batch: 90/1000  Loss: 0.244486\n",
      "Batch: 100/1000  Loss: 0.285163\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from torchvision.models import vgg19\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = T.Compose([T.ToTensor(),\n",
    "                       T.Resize((224,224))])\n",
    "\n",
    "train_set = CIFAR10(root='CIFAR10_data/',\n",
    "                    train=True,\n",
    "                    transform=transform,\n",
    "                    download=True)\n",
    "test_set = CIFAR10(root='CIFAR10_data/',\n",
    "                   train=False,\n",
    "                   transform=transform,\n",
    "                   download=True)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "test_loader = DataLoader(dataset=test_set,\n",
    "                          batch_size=10,\n",
    "                          shuffle=False,\n",
    "                          num_workers=0)\n",
    "net = vgg19(pretrained=True)\n",
    "\n",
    "for param in net.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "net.classifier[6] = torch.nn.Linear(4096, 10)\n",
    "net.to(DEVICE)\n",
    "\n",
    "cel = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "EPOCHS = 10\n",
    "loss_lst = []\n",
    "acc_lst = []\n",
    "start_time = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    batch_time = time.time()\n",
    "    print(f'====== Epoch: {epoch+1:2d} / {EPOCHS} ======')\n",
    "    net.train()\n",
    "    l_sum = 0\n",
    "    for batch_idx, (x,y) in enumerate(train_loader):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        z = net(x)\n",
    "        loss = cel(z, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        l_sum += loss.item()\n",
    "        if (batch_idx+1) % 10 == 0:\n",
    "            print(f'Batch: {batch_idx+1:2d}/{len(train_loader)} ',\n",
    "                f'Loss: {loss.item():0.6f}') \n",
    "                         \n",
    "    loss_lst.append(l_sum/len(train_loader))\n",
    "\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x,y) in enumerate(test_loader):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            z = net(x)\n",
    "            yhat = torch.argmax(z, dim=1)\n",
    "            correct += torch.sum(y==yhat)\n",
    "        \n",
    "    accuracy = correct / len(test_set)\n",
    "    acc_lst.append(accuracy)\n",
    "    print(f'Accuracy: {accuracy.item()*100:0.2f}%')\n",
    "    print(\"elapsed time:\", time.time() - batch_time)\n",
    "        \n",
    "print(\"elapsed time:\", time.time() - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
